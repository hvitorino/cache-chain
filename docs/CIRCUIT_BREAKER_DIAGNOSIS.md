# Diagn√≥stico: Circuit Breakers Abrindo Muito Rapidamente

## üî¥ Problema Observado

Circuit breakers est√£o abrindo em **menos de 1 segundo** ap√≥s receberem carga, causando rejei√ß√£o de 99% das requisi√ß√µes.

**Evid√™ncia dos logs:**
```
15:11:18.386 - L1-Memory:  closed ‚Üí open
15:11:19.291 - L2-Redis:   closed ‚Üí open
15:11:28.389 - L1-Memory:  open ‚Üí half-open ‚Üí closed ‚Üí open (ciclo r√°pido!)
15:11:29.294 - L2-Redis:   open ‚Üí half-open ‚Üí open (falha na recupera√ß√£o)
15:11:53.497 - PostgreSQL: closed ‚Üí open (cascata completa)
```

---

## üîç An√°lise da Configura√ß√£o Atual

### Configura√ß√£o dos Circuit Breakers

```
L1-Memory:   timeout=100ms, max_requests=1, interval=0s, cb_timeout=10s
L2-Redis:    timeout=1s,    max_requests=1, interval=0s, cb_timeout=10s  
PostgreSQL:  timeout=1s,    max_requests=1, interval=0s, cb_timeout=10s
```

### Fun√ß√£o ReadyToTrip (C√≥digo)

```go
// pkg/resilience/config.go - linha 52
ReadyToTrip: func(counts Counts) bool {
    return counts.ConsecutiveFailures >= 5  // ‚Üê Threshold muito baixo!
},
```

---

## üéØ Causas Raiz Identificadas

### 1. **Threshold Muito Baixo (5 falhas consecutivas)**

**Problema:** Com apenas **5 falhas consecutivas**, o circuit breaker abre.

**Cen√°rio real:**
```
Request 1: timeout (timeout=100ms √© muito agressivo para L1-Memory)
Request 2: timeout
Request 3: timeout  
Request 4: timeout
Request 5: timeout
‚Üí Circuit breaker ABRE ap√≥s 500ms de carga!
```

**Por que √© problem√°tico:**
- Em cen√°rios de alta concorr√™ncia, 5 requisi√ß√µes chegam em **milissegundos**
- Uma pequena lat√™ncia causa abertura imediata
- N√£o h√° "tempo de respira√ß√£o" para recupera√ß√£o transit√≥ria

**Recomenda√ß√£o:**
```go
ReadyToTrip: func(counts Counts) bool {
    failureRate := float64(counts.TotalFailures) / float64(counts.Requests)
    return counts.Requests >= 20 && failureRate > 0.5  // 50% de erro em 20+ req
},
```

---

### 2. **Interval=0s (Contador Nunca Reseta)**

**Problema:** `Interval: 0` significa que o contador de falhas **nunca** √© limpo.

**Comportamento:**
```
Tempo 0s:  5 falhas ‚Üí CB abre
Tempo 10s: CB tenta half-open
           1 falha ‚Üí CB abre novamente (contador acumulado!)
Tempo 20s: CB tenta half-open
           1 falha ‚Üí CB abre (falhas anteriores ainda contam!)
```

**Por que √© problem√°tico:**
- Falhas antigas continuam contando indefinidamente
- Uma rajada inicial de erros "condena" o circuit breaker permanentemente
- N√£o h√° "perd√£o" para recupera√ß√£o

**Recomenda√ß√£o:**
```go
Interval: 60 * time.Second,  // Reseta contadores a cada 60s
```

---

### 3. **MaxRequests=1 (Half-Open Muito Fr√°gil)**

**Problema:** No estado **half-open**, apenas **1 requisi√ß√£o** √© permitida para testar recupera√ß√£o.

**Comportamento:**
```
CB em half-open: permite 1 request
‚Üí Se essa request falhar (mesmo por timeout transit√≥rio) ‚Üí CB volta para open
‚Üí Espera mais 10s antes de tentar novamente
```

**Por que √© problem√°tico:**
- **Sample size muito pequeno:** 1 requisi√ß√£o n√£o √© estatisticamente significativo
- Qualquer lat√™ncia moment√¢nea reabre o circuit breaker
- Dificulta recupera√ß√£o em sistemas com jitter natural

**Exemplo do log:**
```
15:11:28.389 - L1-Memory: half-open ‚Üí closed ‚Üí open (em milissegundos!)
```
‚Ü≥ Conseguiu fechar, mas reabriu imediatamente porque o pr√≥ximo request falhou

**Recomenda√ß√£o:**
```go
MaxRequests: 5,  // Permite 5 requests em half-open
```

---

### 4. **Timeout=100ms para L1-Memory (Muito Agressivo)**

**Problema:** Timeout de **100ms** para opera√ß√µes de mem√≥ria √© **ultrarr√°pido**.

**Realidade:**
```
Opera√ß√£o de mem√≥ria normal: 10-50Œºs (microsegundos)
Timeout configurado:        100ms (100,000Œºs)
```

**Por√©m, sob carga:**
- **GC pause:** pode levar 10-50ms
- **Lock contention:** m√∫ltiplas goroutines competindo
- **System scheduler:** atraso de CPU

**Resultado:** Timeouts falsos sob carga moderada.

**Recomenda√ß√£o:**
```go
// L1 (Memory)
timeout: 500 * time.Millisecond,

// L2 (Redis)  
timeout: 2 * time.Second,

// L3 (PostgreSQL)
timeout: 5 * time.Second,
```

---

### 5. **CB Timeout=10s (Recupera√ß√£o Muito R√°pida)**

**Problema:** Circuit breaker tenta reabrir a cada **10 segundos**.

**Ciclo observado:**
```
T=0s:  CB abre (5 falhas)
T=10s: CB tenta half-open
       ‚Üí 1 falha ‚Üí reabre
T=20s: CB tenta half-open
       ‚Üí 1 falha ‚Üí reabre
T=30s: CB tenta half-open
       ‚Üí 1 falha ‚Üí reabre
```

**Por que √© problem√°tico:**
- Sistema sob stress n√£o tem tempo para se estabilizar
- Backend sobrecarregado n√£o teve 10s de "descanso" suficiente
- Cria ciclo de "flapping" (abre/fecha rapidamente)

**Recomenda√ß√£o:**
```go
Timeout: 30 * time.Second,  // Aguarda 30s antes de tentar novamente
```

---

## üß™ Simula√ß√£o: Por Que Abre T√£o R√°pido?

### Cen√°rio: Load Test com 10 req/s

**Configura√ß√£o Atual:**
- Threshold: 5 falhas consecutivas
- Interval: 0s (nunca reseta)
- MaxRequests: 1
- Timeout: 100ms (L1)

**Timeline:**
```
T=0.000s: Request 1 ‚Üí timeout (100ms) ‚Üí Falha 1
T=0.100s: Request 2 ‚Üí timeout (100ms) ‚Üí Falha 2
T=0.200s: Request 3 ‚Üí timeout (100ms) ‚Üí Falha 3
T=0.300s: Request 4 ‚Üí timeout (100ms) ‚Üí Falha 4
T=0.400s: Request 5 ‚Üí timeout (100ms) ‚Üí Falha 5
T=0.500s: üî¥ CIRCUIT BREAKER ABRE!

T=0.501s: Request 6  ‚Üí Rejeitado (CB open)
T=0.502s: Request 7  ‚Üí Rejeitado (CB open)
...
T=10.5s:  CB tenta half-open
          Request 50 ‚Üí 1 falha ‚Üí üî¥ REABRE!
```

**Resultado:** Circuit breaker aberto **99% do tempo** ap√≥s 500ms de carga!

---

## üìä Compara√ß√£o: Atual vs Recomendado

| Par√¢metro | Atual | Recomendado | Impacto |
|-----------|-------|-------------|---------|
| **Threshold** | 5 falhas consecutivas | 50% error rate em 20+ req | 4x mais resiliente |
| **Interval** | 0s (nunca reseta) | 60s | Permite recupera√ß√£o |
| **MaxRequests** | 1 | 5 | 5x mais confian√ßa |
| **Timeout L1** | 100ms | 500ms | 5x mais tolerante |
| **Timeout L2** | 1s | 2s | 2x mais tolerante |
| **CB Timeout** | 10s | 30s | 3x mais tempo para estabilizar |

---

## ‚úÖ Solu√ß√£o Recomendada

### Op√ß√£o 1: Configura√ß√£o Conservadora (Produ√ß√£o)

```go
// pkg/resilience/config.go
func DefaultResilientConfig() ResilientConfig {
    return ResilientConfig{
        Timeout: 5 * time.Second,
        CircuitBreakerConfig: CircuitBreakerConfig{
            MaxRequests: 5,                    // ‚Üê 5 requests em half-open
            Interval:    60 * time.Second,     // ‚Üê Reseta a cada 60s
            Timeout:     30 * time.Second,     // ‚Üê Aguarda 30s antes de reabrir
            ReadyToTrip: func(counts Counts) bool {
                // Abre se >50% de erro ap√≥s 20+ requisi√ß√µes
                if counts.Requests < 20 {
                    return false
                }
                failureRate := float64(counts.TotalFailures) / float64(counts.Requests)
                return failureRate > 0.5
            },
        },
    }
}
```

**Timeouts por camada (chain.go):**
```go
if i == 0 {
    // L1 - Memory
    resConfig = resConfig.WithTimeout(500 * time.Millisecond)
} else if i == 1 {
    // L2 - Redis
    resConfig = resConfig.WithTimeout(2 * time.Second)
} else {
    // L3+ - PostgreSQL, etc
    resConfig = resConfig.WithTimeout(5 * time.Second)
}
```

---

### Op√ß√£o 2: Configura√ß√£o Agressiva (Performance Cr√≠tica)

Para cen√°rios onde prefer√™ncia √© **fail fast** com recupera√ß√£o r√°pida:

```go
CircuitBreakerConfig{
    MaxRequests: 3,
    Interval:    30 * time.Second,
    Timeout:     15 * time.Second,
    ReadyToTrip: func(counts Counts) bool {
        // Mais agressivo: 60% de erro em 10+ req
        if counts.Requests < 10 {
            return false
        }
        failureRate := float64(counts.TotalFailures) / float64(counts.Requests)
        return failureRate > 0.6
    },
}
```

---

### Op√ß√£o 3: Configura√ß√£o por Camada (H√≠brida)

```go
// No main.go
l1Config := resilience.DefaultResilientConfig()
l1Config.Timeout = 500 * time.Millisecond
l1Config.CircuitBreakerConfig.Timeout = 20 * time.Second

l2Config := resilience.DefaultResilientConfig()
l2Config.Timeout = 2 * time.Second
l2Config.CircuitBreakerConfig.Timeout = 30 * time.Second

l3Config := resilience.DefaultResilientConfig()
l3Config.Timeout = 5 * time.Second
l3Config.CircuitBreakerConfig.Timeout = 60 * time.Second

cacheChain, err := chain.NewWithConfig(
    chain.ChainConfig{
        ResilientConfigs: []resilience.ResilientConfig{
            l1Config,
            l2Config,
            l3Config,
        },
        // ...
    },
    memCache, redisCache, pgAdapter,
)
```

---

## üéØ Implementa√ß√£o Imediata

### Mudan√ßas M√≠nimas (Quick Fix)

Editar apenas `pkg/resilience/config.go`:

```go
func DefaultResilientConfig() ResilientConfig {
    return ResilientConfig{
        Timeout: 5 * time.Second,
        CircuitBreakerConfig: CircuitBreakerConfig{
            MaxRequests: 5,                    // era: 1
            Interval:    60 * time.Second,     // era: 0
            Timeout:     30 * time.Second,     // era: 10s
            ReadyToTrip: func(counts Counts) bool {
                // Nova l√≥gica: error rate em vez de falhas consecutivas
                if counts.Requests < 20 {
                    return false
                }
                failureRate := float64(counts.TotalFailures) / float64(counts.Requests)
                return failureRate > 0.5       // era: >= 5 falhas
            },
        },
    }
}
```

**Resultado esperado:**
- Circuit breaker aguarda 20+ requisi√ß√µes antes de decidir
- Tolera at√© 50% de falhas (10 em 20) antes de abrir
- Reseta contadores a cada 60s (permite recupera√ß√£o de rajadas)
- Testa com 5 requisi√ß√µes em half-open (mais confian√ßa)
- Aguarda 30s antes de retentar (sistema tem tempo de estabilizar)

---

## üìà Valida√ß√£o P√≥s-Mudan√ßa

### M√©tricas para Monitorar

```promql
# Taxa de abertura de circuit breakers (deve reduzir drasticamente)
rate(banking_api_circuit_opens_total[5m])

# Estado dos circuit breakers (0=closed, 1=open, 2=half-open)
banking_api_circuit_state

# Taxa de erros (deve aumentar inicialmente, depois estabilizar)
rate(banking_api_cache_errors_total[1m])

# Lat√™ncia (deve permanecer est√°vel)
histogram_quantile(0.95, banking_api_cache_get_latency_seconds)
```

### Logs para Verificar

```bash
# Frequ√™ncia de mudan√ßas de estado (deve reduzir)
docker logs banking-api 2>&1 | grep "circuit breaker state changed" | wc -l

# Tempo entre open ‚Üí half-open (deve ser 30s)
docker logs banking-api 2>&1 | grep "half-open" | tail -10

# Erros por tipo (identificar verdadeiros problemas)
docker logs banking-api 2>&1 | jq -r 'select(.error_type) | .error_type' | sort | uniq -c
```

---

## üö® Sinais de Sucesso

### Antes (Problema):
```
‚úó Circuit breakers abrem em <1s sob carga
‚úó Ciclo: open ‚Üí half-open ‚Üí open (flapping)
‚úó 99% de requisi√ß√µes rejeitadas
‚úó Recupera√ß√£o imposs√≠vel
```

### Depois (Corrigido):
```
‚úì Circuit breakers permanecem closed sob carga normal
‚úì Abrem apenas quando >50% de erro real (20+ req)
‚úì Half-open testa com 5 requisi√ß√µes (sample size adequado)
‚úì Recupera√ß√£o gradual e est√°vel
‚úì Contadores resetam a cada 60s (permite recupera√ß√£o)
```

---

## üìö Refer√™ncias

- **gobreaker Documentation**: https://github.com/sony/gobreaker
- **Circuit Breaker Pattern**: Martin Fowler - https://martinfowler.com/bliki/CircuitBreaker.html
- **SRE Best Practices**: Google SRE Book - Handling Overload
- **C√≥digo Fonte**: `pkg/resilience/config.go`, `pkg/resilience/layer.go`
